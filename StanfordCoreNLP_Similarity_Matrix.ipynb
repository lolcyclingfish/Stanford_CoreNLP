{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW3\n",
    "\n",
    "http://www.klintonbicknell.com/ling400fall2017/hw/hw3.html\n",
    "\n",
    "#before calling server in python, need to start server in terminal within the directory where Stanford Corenlp sorce folder is located at\n",
    "#source folder: C:\\Users\\Dingding\\stanford-corenlp-full-2017-06-09\n",
    "#command line: java -Xmx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000\n",
    "\n",
    "#ref.:https://stanfordnlp.github.io/CoreNLP/corenlp-server.html\n",
    "#ref.:https://stanfordnlp.github.io/CoreNLP/corenlp-server.html#starting-the-server\n",
    "#ref.:https://github.com/smilli/py-corenlp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document similarity\n",
    "Calculating document similarity is the first step in many text analytic tasks, including clustering and finding similar documents to a single one known to be of relevance. In this part of the assignment, you’ll compare different ways of computing document similarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pycorenlp import StanfordCoreNLP\n",
    "nlp = StanfordCoreNLP('http://localhost:9000')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.) Splitting, tokenization, normalization. \n",
    "In order to have a set of documents to calculate similarities between, split the classbios.txt file into one file per person. (Note that you can use regular expressions to do this.) Then, tokenize and normalize these files using any of the methods (in Lucene or CoreNLP) you used on the previous assignment. Next, remove stopwords and any words that contain non-alphabetic characters (e.g., puncutation or numbers), using any method you like. [No need to say anything about this problem in your write-up, but do include the code in your zip.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_file=open('classbios.txt', 'r',encoding=\"utf8\")\n",
    "text_file=text_file.read()\n",
    "#print(text_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tokenize and normalize\n",
    "text_split = nlp.annotate(text_file, properties={\n",
    "  'annotators': 'tokenize, lemma',\n",
    "  'outputFormat': 'json'\n",
    "  })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#text_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#join split lemmas into list_of_tokens:\n",
    "tokens=[]\n",
    "for s in text_split['sentences']:\n",
    "    tokens_from_this_s=[lemma['lemma'] for lemma in s['tokens']]\n",
    "    tokens.append(tokens_from_this_s)\n",
    "\n",
    "#tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#remove stop words using nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "reduced_tokens=[]\n",
    "stopset = ['I','a','and','to','the','in','of','my','for','with','that','as','at','from','is','on','have','me','be','an','it','this',\n",
    "          ',', '.', '------------------------------------------------------------', ':', '**']\n",
    "cachedStopWords = stopset+stopwords.words(\"english\")\n",
    "\n",
    "for each in tokens:\n",
    "    each=[token for token in each if token not in cachedStopWords]\n",
    "    reduced_tokens.append(each)\n",
    "\n",
    "#reduced_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#join the list of sentence'tokens into one big list of all tokens:\n",
    "flat_reduced_tokens=[]\n",
    "for token_sentence in reduced_tokens:\n",
    "    flat_reduced_tokens=flat_reduced_tokens+[token for token in token_sentence]\n",
    "\n",
    "#flat_reduced_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# split list of all tokens into .txt file docs by 'hw0' for each classmate\n",
    "tokens_one_doc=[]\n",
    "list_docs=[]\n",
    "flat_reduced_tokens.append('hw0')\n",
    "for token in flat_reduced_tokens:\n",
    "    if token != 'hw0':\n",
    "        tokens_one_doc.append(token) #keep appending tokens from one doc to the same doc\n",
    "    else:\n",
    "        if len(tokens_one_doc)>0:\n",
    "            list_docs.append(tokens_one_doc) #append completed doc to list of docs\n",
    "            tokens_one_doc=[] #reset doc\n",
    "\n",
    "#list_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#write list of docs out to local txt files:\n",
    "import os\n",
    "\n",
    "for doc in list_docs: \n",
    "    if len(doc)!=0:\n",
    "        doc_text= \" \".join([token for token in doc])\n",
    "        with open(os.path.join('C:\\\\Users\\Dingding\\class_bios_docs',(doc[0]+' '+doc[1]).lstrip()+'.txt'), \"w\",encoding=\"utf-8\") as file: \n",
    "            file.write(doc_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.) Boolean similarity. \n",
    "\n",
    "Use the normalized, tokenized bio documents to create a binary term-document matrix, where each row represents a vocabulary item, each column represents a document, and each element is a 0 or 1. In this matrix, each document is represented by a binary vector. From this binary matrix, create a document similarity matrix, where cell i, j gives the similarity of documents i and j, and where similarity is given by the cosine between the document vectors: I⋅J|I||J|I⋅J|I||J|. Save this matrix to a file boolean.txt, where values are separated by a space, and where the rows and columns are arranged in the same order they were given in classbios.txt. Do not include any row or column labels in this file. In your write-up, answer: Which three pairs of documents are the most similar? Which three pairs are the least similar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "first_token = list_docs[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_Token = 0\n",
    "token_dict = {first_token:n_Token}\n",
    "n_Token += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_Doc = len(list_docs)\n",
    "boolMatrix = np.zeros([1,n_Doc])\n",
    "boolMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 1.,  1.,  0., ...,  0.,  1.,  1.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  1.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# edit boolean matrix\n",
    "idx_Doc = 0\n",
    "for doc in list_docs:\n",
    "    for token in doc:\n",
    "        \n",
    "        if token not in token_dict.keys(): # add a new token\n",
    "            token_dict[token] = n_Token\n",
    "            boolMatrix = np.concatenate([boolMatrix,np.zeros([1,n_Doc])])\n",
    "            n_Token += 1\n",
    "        # register token into boolMatrix\n",
    "        idx_Token = token_dict[token]\n",
    "        boolMatrix[idx_Token,idx_Doc] = 1\n",
    "            \n",
    "    idx_Doc += 1 # go to next bio\n",
    "\n",
    "boolMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BOOLEAN SIMILARITY\n",
    "boolSimMatrix = np.zeros([n_Doc,n_Doc])\n",
    "boolSimMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top3Value = np.array([-1,-1,-1.0]); \n",
    "top3Idx = [-1,-1,-1]; # ordered\n",
    "bottom3Value = np.array([2,2,2.0]); \n",
    "bottom3Idx = [-1,-1,-1]; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.00117782,  0.00099433, ...,  0.00142954,\n",
       "         0.00070458,  0.00077039],\n",
       "       [ 0.00117782,  0.        ,  0.00097001, ...,  0.00138351,\n",
       "         0.00091646,  0.00089928],\n",
       "       [ 0.00099433,  0.00097001,  0.        , ...,  0.0011236 ,\n",
       "         0.00093037,  0.00090289],\n",
       "       ..., \n",
       "       [ 0.00142954,  0.00138351,  0.0011236 , ...,  0.        ,\n",
       "         0.0011269 ,  0.00106456],\n",
       "       [ 0.00070458,  0.00091646,  0.00093037, ...,  0.0011269 ,\n",
       "         0.        ,  0.00085305],\n",
       "       [ 0.00077039,  0.00089928,  0.00090289, ...,  0.00106456,\n",
       "         0.00085305,  0.        ]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(n_Doc):\n",
    "    for j in range(n_Doc):\n",
    "        \n",
    "        if j == i:\n",
    "            continue\n",
    "        if j < i: # similarity has been calculated\n",
    "            boolSimMatrix[i,j] = boolSimMatrix[j,i]\n",
    "        else:\n",
    "            \n",
    "            Vec_i = boolMatrix[:,i]; Vec_j = boolMatrix[:,j]\n",
    "            \n",
    "            Sim_ij = np.sum(Vec_i*Vec_j)/(np.sum(Vec_i)*np.sum(Vec_j))\n",
    "            boolSimMatrix[i,j] = Sim_ij\n",
    "boolSimMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write matrix into a txt file\n",
    "np.savetxt('boolean.txt',boolSimMatrix,fmt='%.5f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### top three similar pair of bios in cosine distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       ..., \n",
       "       [False, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False]], dtype=bool)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get top three and bottom three values in boolean similarity matrix:\n",
    "#get top three values:\n",
    "idx_array_top3value=boolSimMatrix.max(0).argsort()[-4:][::-1] #top three value index in array of top value in each document\n",
    "top3value=boolSimMatrix.max(0)[idx_array_top3value].tolist()\n",
    "#mark top three values in boolean similarity matrix:\n",
    "top3_boolMatrix=np.isin(boolSimMatrix, top3value)\n",
    "top3_boolMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 7, 20, 22, 33], dtype=int64), array([33, 22, 20,  7], dtype=int64))"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get index of top three values:\n",
    "top3_index=np.where(top3_boolMatrix) #first array is row index, second array is column index\n",
    "top3_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEGHAN MENGHAN DING originally Shenzhen southern part China come United State senior year high school Rochester NY 2009 complete undergraduate study Northwestern 2014 degree statistics Economics go work investment management project management 2 year decide take technical path business world come back Northwestern msia program become interested text analytic since join msia program hear term learn see people present work text analytic either class industry amaze amount value extract unstructured datum want learn powerful tool future plan career datum science especially decision science side believe text analytic natural language processing one major distinguishing point make better datum scientist another major reason decide text analytic class pick python MSiA fall love elegant language want become better\n",
      " \n",
      "CHAORAN WEI name Chaoran Wei MSiA program China undergraduate research project machine learning interested idea solve real world problem datum choose go MSiA get master degree want become data scientist future investigate machine learning two year come MSiA biggest research project attempt classify image datum plankton specific species primary method group use deep Convolutional Neural Networks -lrb- CNN -rrb- research result one conference presentation JMM -lrb- Joint Mathematics meeting -rrb- special session try variety text analytic algorithm undergraduate study use topic modeling technique extract different topic research papers International Relations realm result exciting less interpretable time use LSTM try generate text Wall Street Journal turn model could generate meaningful word even day training try bunch method get good interpretable result part reason want take course get idea model use particular text analytic problem use delving deeper theory nlp future plan become data scientist like idea extract meaningful information pattern large dataset bring business value company feel like solve puzzle money increasingly amount text datum diverse source present web believe text analytic class quarter serve well future career\n",
      " \n",
      "ARON LIU\n",
      " \n",
      "MING YU LIU Hi name Ethen MSIA program prior come MSIA program undergrad back Taipei Taiwan Industrial Management National Taiwan University Science Technology reason come program start take couple computer science analytic course junior year find enjoy express logic code work datum derive insight little detour major make certain want learn analytic decide wish pursue advanced degree related field bit research feel like MSIA aim program strike good balance theory apply part datum science thus struggle GRE test apply program thankfully accept become interested text analytic come across method deal cold start problem recommendation system -lrb- note cold start basically user product interact system thus leverage canonical machine learn algorithm prior history work -rrb- scenario product 's item description use feature provide additional side information task make problem still feasible solve realize work knowledge perform text analytic highly beneficial field interested plan near future aim machine learn engineer technical datum scientist role field recommendation system get year industrial hands-on experience technical role maybe consider transition business-oriented datum science role\n",
      " \n",
      "MING YU LIU Hi name Ethen MSIA program prior come MSIA program undergrad back Taipei Taiwan Industrial Management National Taiwan University Science Technology reason come program start take couple computer science analytic course junior year find enjoy express logic code work datum derive insight little detour major make certain want learn analytic decide wish pursue advanced degree related field bit research feel like MSIA aim program strike good balance theory apply part datum science thus struggle GRE test apply program thankfully accept become interested text analytic come across method deal cold start problem recommendation system -lrb- note cold start basically user product interact system thus leverage canonical machine learn algorithm prior history work -rrb- scenario product 's item description use feature provide additional side information task make problem still feasible solve realize work knowledge perform text analytic highly beneficial field interested plan near future aim machine learn engineer technical datum scientist role field recommendation system get year industrial hands-on experience technical role maybe consider transition business-oriented datum science role\n",
      " \n",
      "ARON LIU\n",
      " \n",
      "CHAORAN WEI name Chaoran Wei MSiA program China undergraduate research project machine learning interested idea solve real world problem datum choose go MSiA get master degree want become data scientist future investigate machine learning two year come MSiA biggest research project attempt classify image datum plankton specific species primary method group use deep Convolutional Neural Networks -lrb- CNN -rrb- research result one conference presentation JMM -lrb- Joint Mathematics meeting -rrb- special session try variety text analytic algorithm undergraduate study use topic modeling technique extract different topic research papers International Relations realm result exciting less interpretable time use LSTM try generate text Wall Street Journal turn model could generate meaningful word even day training try bunch method get good interpretable result part reason want take course get idea model use particular text analytic problem use delving deeper theory nlp future plan become data scientist like idea extract meaningful information pattern large dataset bring business value company feel like solve puzzle money increasingly amount text datum diverse source present web believe text analytic class quarter serve well future career\n",
      " \n",
      "MEGHAN MENGHAN DING originally Shenzhen southern part China come United State senior year high school Rochester NY 2009 complete undergraduate study Northwestern 2014 degree statistics Economics go work investment management project management 2 year decide take technical path business world come back Northwestern msia program become interested text analytic since join msia program hear term learn see people present work text analytic either class industry amaze amount value extract unstructured datum want learn powerful tool future plan career datum science especially decision science side believe text analytic natural language processing one major distinguishing point make better datum scientist another major reason decide text analytic class pick python MSiA fall love elegant language want become better\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#retrieve top 3 pairs of most similar bios according to cosine distance: returned four pair of indexes (tie situation)\n",
    "for x in range(0,4):\n",
    "    #print(top3value[x]) #cosine similarity score\n",
    "    print(\" \".join(list_docs[top3_index[0][x]]))\n",
    "    print(\" \")\n",
    "    print(\" \".join(list_docs[top3_index[1][x]]))\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bottom three similar pair of bios in cosine distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       ..., \n",
       "       [False, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False]], dtype=bool)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get bottom three and bottom three values in boolean similarity matrix:\n",
    "#get bottom three values:\n",
    "idx_array_bottom3value=boolSimMatrix.min(0).argsort()[-3:] #bottom three value index in array of top value in each document\n",
    "bottom3value=boolSimMatrix.max(0)[idx_array_bottom3value].tolist()\n",
    "#mark bottom three values in boolean similarity matrix:\n",
    "bottom3_boolMatrix=np.isin(boolSimMatrix, bottom3value)\n",
    "bottom3_boolMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 7,  7, 15, 23, 34, 35], dtype=int64),\n",
       " array([15, 35,  7, 34, 23,  7], dtype=int64))"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get index of bottom three values:\n",
    "bottom3_index=np.where(bottom3_boolMatrix) #first array is row index, second array is column index\n",
    "bottom3_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEGHAN MENGHAN DING originally Shenzhen southern part China come United State senior year high school Rochester NY 2009 complete undergraduate study Northwestern 2014 degree statistics Economics go work investment management project management 2 year decide take technical path business world come back Northwestern msia program become interested text analytic since join msia program hear term learn see people present work text analytic either class industry amaze amount value extract unstructured datum want learn powerful tool future plan career datum science especially decision science side believe text analytic natural language processing one major distinguishing point make better datum scientist another major reason decide text analytic class pick python MSiA fall love elegant language want become better\n",
      " \n",
      "SIK JUNG Seoul Korea grow since 15 move Shanghai China attend high school move Los Angeles CA attend UCLA Evanston il come msia want datum science field first hear field take statistics econometric course UCLA get fascinate want learn work take datum science course online apply graduate program fully immerse datum science Northwestern visit friend Evanston summer 2015 get visit Northwestern campus think beautiful little know bad winter come MSiA work consultant Deloitte Los Angeles work financial transaction group accounting consulting initial public offering merger acquisition type financial transaction even though non-data-science work opportunity work Deloitte Analytics group use SAS analyze transaction datum various client get interested text analytic take Professor Diego 's class fall use various text analytic method -- regular expression topic modeling tf-idf latent dirichlet allocation -- parse analyze email receive msia use method cluster email recommend past answer future email also practicum gogo use various text analytic method word2vec cluster text immediate plan find company interesting datum product passionate see take\n",
      " \n",
      "MEGHAN MENGHAN DING originally Shenzhen southern part China come United State senior year high school Rochester NY 2009 complete undergraduate study Northwestern 2014 degree statistics Economics go work investment management project management 2 year decide take technical path business world come back Northwestern msia program become interested text analytic since join msia program hear term learn see people present work text analytic either class industry amaze amount value extract unstructured datum want learn powerful tool future plan career datum science especially decision science side believe text analytic natural language processing one major distinguishing point make better datum scientist another major reason decide text analytic class pick python MSiA fall love elegant language want become better\n",
      " \n",
      "KEVIN ZHAI year ago dad study English college Nanjing China one day history professor US come college give lecture dad choose translator lecture professor dad talk one reason another dad decide get phd history States professor finish phd find job Montgomery AL professor move whole family new land end bear Montgomery fast forward 18 year -- 2011 time pick college live anywhere quaint little city whole life know time pack grit tooth elsewhere end UPenn study Materials Science choose major ? probably wanderlust -- think look cool really know else fancy interest time graduate reazly work lab run simulation molecule calling want leverage computer science skill decide datum science right path apply graduate school right bachelor luckily get accept great program Northwestern first quarter take survey class look various use case capability analytic specifically team project topic modelling corpus research papers look different topic evolve year first foray power text crunching nlp amaze powerful certainly pitfall methodology show lot promise term future plan world oyster patron seafood restaurant spice thing -lrb- since hit 300 minimum word count already -rrb- decide insert excerpt Kama Sutra `` make one attractive appearance quality age liberality paste rosebay wild ginger plum leave make one seductive salve eye make grind herb put powder wick burn myrobalan oil human skull ... lick paste honey ghee mixed powder dry red blue lotus rise chestnut make person attractive ... eye peacock hyena put inside locket gold wear right hand also render one attractive place bristle certain insect bear tree penis massage oil ten night repeat make penis swell lie face downward string cot let penis hang process conclude gradually relieve pain cold salve swell last life ''\n",
      " \n",
      "SIK JUNG Seoul Korea grow since 15 move Shanghai China attend high school move Los Angeles CA attend UCLA Evanston il come msia want datum science field first hear field take statistics econometric course UCLA get fascinate want learn work take datum science course online apply graduate program fully immerse datum science Northwestern visit friend Evanston summer 2015 get visit Northwestern campus think beautiful little know bad winter come MSiA work consultant Deloitte Los Angeles work financial transaction group accounting consulting initial public offering merger acquisition type financial transaction even though non-data-science work opportunity work Deloitte Analytics group use SAS analyze transaction datum various client get interested text analytic take Professor Diego 's class fall use various text analytic method -- regular expression topic modeling tf-idf latent dirichlet allocation -- parse analyze email receive msia use method cluster email recommend past answer future email also practicum gogo use various text analytic method word2vec cluster text immediate plan find company interesting datum product passionate see take\n",
      " \n",
      "MEGHAN MENGHAN DING originally Shenzhen southern part China come United State senior year high school Rochester NY 2009 complete undergraduate study Northwestern 2014 degree statistics Economics go work investment management project management 2 year decide take technical path business world come back Northwestern msia program become interested text analytic since join msia program hear term learn see people present work text analytic either class industry amaze amount value extract unstructured datum want learn powerful tool future plan career datum science especially decision science side believe text analytic natural language processing one major distinguishing point make better datum scientist another major reason decide text analytic class pick python MSiA fall love elegant language want become better\n",
      " \n",
      "KRISTIN MEIER name Kristin Meier grow Long Island New York attend George Washington University major Economics minor Mathematics Statistics throughout study become increasingly interested learn use data analysis solve real-world problem graduate work Research Assistant Federal Reserve Board Governors Washington D.C. Federal Reserve opportunity work many project inspire interest datum analytic become interested programming work r want expand data analysis toolkit seek build educational foundation work experience classroom practical application master science Analytics program offer perfect opportunity advance skill combine several subject area interest recently think much quantify textual datum briefly learn technique become interested text analytic expand knowledge subject lot unstructured datum world extremely beneficial know extract information use employer advantage internship summer policy research organization learn interesting project use text analytic classify domestic abuse case crisis text hotline datum application text analytic immense future plan include find job datum science enable contribute business value company hope work somewhere continue grow data scientist polish machine learn skill excite opportunity lie ahead free time enjoy travel sport explore new area city\n",
      " \n",
      "LAUREN YU name Lauren come central China MSiA math major SUNY Buffalo dabble lot social science discover datum science/analytics quantitative social science research lot statistical method deploy although personally engage text analytic project time definitely significant research work interest area Social science offer great range interesting problem involve natural language processing instance text tell author 's intent bias ? measure much therapist empathize patient ? University Washington 's NLP group example collaborate widely political scientist psychologist social scientist tackle kind question remember see paper title `` contextualize sarcasm detection Twitter '' cool ! Sarcasm could subtle nature depend highly subject matter -- many case almost like insider joke able detect algorithmically sound like great idea future hope work project enjoy live city decent public transportation lot korean food earn enough money actually adopt panda bear watch panda video probably biggest addiction something hypnotize bear even roll munch bamboo sleep play keeper play play snow play toy nothing general Pandas spend 14 hour day sleep eat 20lb bamboo shoot day best lifestyle eat sleep repeat ! Ok stop troll reach 300 word already point !\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#retrieve  bottom 3 pairs of most similar bios according to cosine distance: returned four pair of indexes (tie situation)\n",
    "for x in range(0,4):\n",
    "    #print(bottom3value[x]) #cosine similarity score\n",
    "    print(\" \".join(list_docs[bottom3_index[0][x]]))\n",
    "    print(\" \")\n",
    "    print(\" \".join(list_docs[bottom3_index[1][x]]))\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf-idf similarity\n",
    "\n",
    "Use the normalized, tokenized bio documents to create a term-document matrix of raw frequencies. Then transform this raw frequency matrix into a tf-idf matrix by using the formula (1+log(tf))(log(Ndf))(1+log⁡(tf))(log⁡(Ndf)) where tftf is the term’s raw frequency in this document, dfdf is the number of documents the term occurs in, and NN is the total number of documents. Now, again, convert this term-document matrix into a document similarity matrix in the same way as before and save it as tf_idf.txt, in the same format as before. In your write-up, answer: Which three pairs of documents are the most similar now? Which three pairs are the least similar? Does this more advanced method seem to be doing a better job than just using binary similarity? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_Token\n",
    "n_Doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqMatrix = np.zeros([n_Token,n_Doc])\n",
    "freqMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 4.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 2.,  2.,  0., ...,  0.,  4.,  4.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  2.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  2.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  2.]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# edit boolean matrix\n",
    "idx_Doc = 0\n",
    "for doc in list_docs:\n",
    "    for token in doc:\n",
    "        # register token into boolMatrix\n",
    "        idx_Token = token_dict[token] \n",
    "        freqMatrix[idx_Token,idx_Doc] += 1\n",
    "            \n",
    "    idx_Doc += 1 # go to next bio\n",
    "freqMatrix        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# frequency matrix to tf-idf matrix\n",
    "tfidfMatrix = np.zeros([n_Token,n_Doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 8.55133104,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 1.37302421,  1.37302421,  0.        , ...,  0.        ,\n",
       "         1.9351182 ,  1.9351182 ],\n",
       "       ..., \n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  6.06742499],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  6.06742499],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  6.06742499]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for idx_Token in range(n_Token):\n",
    "    for idx_Doc in range(n_Doc):\n",
    "        \n",
    "        tf = freqMatrix[idx_Token,idx_Doc]   \n",
    "        \n",
    "        if tf == 0:\n",
    "            continue\n",
    "        df = (freqMatrix[idx_Token,:]>0.1).sum()\n",
    "        tfidfMatrix[idx_Token,idx_Doc] = (1 + np.log(tf)) * np.log(n_Doc/float(df))\n",
    "tfidfMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initiate tfidf similarity matrix\n",
    "tfidfSimMatrix = np.zeros([n_Doc,n_Doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.00037603,  0.00037602, ...,  0.0004653 ,\n",
       "         0.0001906 ,  0.00015558],\n",
       "       [ 0.00037603,  0.        ,  0.00044632, ...,  0.00044872,\n",
       "         0.00029046,  0.00022938],\n",
       "       [ 0.00037602,  0.00044632,  0.        , ...,  0.00036112,\n",
       "         0.00032444,  0.00027729],\n",
       "       ..., \n",
       "       [ 0.0004653 ,  0.00044872,  0.00036112, ...,  0.        ,\n",
       "         0.00051106,  0.00031779],\n",
       "       [ 0.0001906 ,  0.00029046,  0.00032444, ...,  0.00051106,\n",
       "         0.        ,  0.0003558 ],\n",
       "       [ 0.00015558,  0.00022938,  0.00027729, ...,  0.00031779,\n",
       "         0.0003558 ,  0.        ]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(n_Doc):\n",
    "    for j in range(n_Doc):        \n",
    "        if j == i:\n",
    "            continue\n",
    "        if j < i: # similarity has been calculated\n",
    "            tfidfSimMatrix[i,j] = tfidfSimMatrix[j,i]\n",
    "        else:\n",
    "            Vec_i = tfidfMatrix[:,i]; Vec_j = tfidfMatrix[:,j]\n",
    "            sim_ij = np.sum(Vec_i*Vec_j)/(np.sum(Vec_i)*np.sum(Vec_j))\n",
    "            tfidfSimMatrix[i,j] = sim_ij\n",
    "tfidfSimMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write ifidf similarity matrix into a txt file\n",
    "np.savetxt('tf_idf.txt',tfidfSimMatrix,fmt='%.5f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bottom three similar pair of bios in cosine distance for tf-idf matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       ..., \n",
       "       [False, False, False, ..., False,  True, False],\n",
       "       [False, False, False, ...,  True, False, False],\n",
       "       [False, False, False, ..., False, False, False]], dtype=bool)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get bottom three and bottom three values in tfidf similarity matrix:\n",
    "#get bottom three values:\n",
    "idx_array_bottom3value=tfidfSimMatrix.min(0).argsort()[-3:] #bottom three value index in array of top value in each document\n",
    "bottom3value=tfidfSimMatrix.max(0)[idx_array_bottom3value].tolist()\n",
    "#mark bottom three values in boolean similarity matrix:\n",
    "bottom3_tfidfMatrix=np.isin(tfidfSimMatrix, bottom3value)\n",
    "bottom3_tfidfMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 7, 15, 28, 33, 34, 35], dtype=int64),\n",
       " array([35, 28, 15, 34, 33,  7], dtype=int64))"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get index of bottom three values:\n",
    "bottom3_index=np.where(bottom3_tfidfMatrix) #first array is row index, second array is column index\n",
    "bottom3_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEGHAN MENGHAN DING originally Shenzhen southern part China come United State senior year high school Rochester NY 2009 complete undergraduate study Northwestern 2014 degree statistics Economics go work investment management project management 2 year decide take technical path business world come back Northwestern msia program become interested text analytic since join msia program hear term learn see people present work text analytic either class industry amaze amount value extract unstructured datum want learn powerful tool future plan career datum science especially decision science side believe text analytic natural language processing one major distinguishing point make better datum scientist another major reason decide text analytic class pick python MSiA fall love elegant language want become better\n",
      " \n",
      "KEVIN ZHAI year ago dad study English college Nanjing China one day history professor US come college give lecture dad choose translator lecture professor dad talk one reason another dad decide get phd history States professor finish phd find job Montgomery AL professor move whole family new land end bear Montgomery fast forward 18 year -- 2011 time pick college live anywhere quaint little city whole life know time pack grit tooth elsewhere end UPenn study Materials Science choose major ? probably wanderlust -- think look cool really know else fancy interest time graduate reazly work lab run simulation molecule calling want leverage computer science skill decide datum science right path apply graduate school right bachelor luckily get accept great program Northwestern first quarter take survey class look various use case capability analytic specifically team project topic modelling corpus research papers look different topic evolve year first foray power text crunching nlp amaze powerful certainly pitfall methodology show lot promise term future plan world oyster patron seafood restaurant spice thing -lrb- since hit 300 minimum word count already -rrb- decide insert excerpt Kama Sutra `` make one attractive appearance quality age liberality paste rosebay wild ginger plum leave make one seductive salve eye make grind herb put powder wick burn myrobalan oil human skull ... lick paste honey ghee mixed powder dry red blue lotus rise chestnut make person attractive ... eye peacock hyena put inside locket gold wear right hand also render one attractive place bristle certain insect bear tree penis massage oil ten night repeat make penis swell lie face downward string cot let penis hang process conclude gradually relieve pain cold salve swell last life ''\n",
      " \n",
      "SIK JUNG Seoul Korea grow since 15 move Shanghai China attend high school move Los Angeles CA attend UCLA Evanston il come msia want datum science field first hear field take statistics econometric course UCLA get fascinate want learn work take datum science course online apply graduate program fully immerse datum science Northwestern visit friend Evanston summer 2015 get visit Northwestern campus think beautiful little know bad winter come MSiA work consultant Deloitte Los Angeles work financial transaction group accounting consulting initial public offering merger acquisition type financial transaction even though non-data-science work opportunity work Deloitte Analytics group use SAS analyze transaction datum various client get interested text analytic take Professor Diego 's class fall use various text analytic method -- regular expression topic modeling tf-idf latent dirichlet allocation -- parse analyze email receive msia use method cluster email recommend past answer future email also practicum gogo use various text analytic method word2vec cluster text immediate plan find company interesting datum product passionate see take\n",
      " \n",
      "ERIC PAWLAKOS bear raise Bay Area grow Walnut Creek move around 8th grade bring msia interest statistics undergraduate major Math/Econ find interesting class statistics econometric opportunity within financial industry interested learn data analysis opportunity explore industry program undergraduate student UCLA study math/econ also intern financial firm San Francisco write company valuation report -lrb- potential growth experience management -rrb- determine company invest become interested text analytic one area class program cover also due large amount datum store text seem text analytic quite important data scientist know plan future find job ideally location since spend much time east coast look opportunity need include additional stuff hit 300-600 word limit bit attend UCDavis 2 year UCLA switch major 4 time Davis find campus UCLA one favorite campus play rugby throughout high school college stop play UCLA play guitar since high school mainly casually fun two sibling ; older younger brother older one go college uc Berkeley since graduate younger one start high school\n",
      " \n",
      "ERIC PAWLAKOS bear raise Bay Area grow Walnut Creek move around 8th grade bring msia interest statistics undergraduate major Math/Econ find interesting class statistics econometric opportunity within financial industry interested learn data analysis opportunity explore industry program undergraduate student UCLA study math/econ also intern financial firm San Francisco write company valuation report -lrb- potential growth experience management -rrb- determine company invest become interested text analytic one area class program cover also due large amount datum store text seem text analytic quite important data scientist know plan future find job ideally location since spend much time east coast look opportunity need include additional stuff hit 300-600 word limit bit attend UCDavis 2 year UCLA switch major 4 time Davis find campus UCLA one favorite campus play rugby throughout high school college stop play UCLA play guitar since high school mainly casually fun two sibling ; older younger brother older one go college uc Berkeley since graduate younger one start high school\n",
      " \n",
      "SIK JUNG Seoul Korea grow since 15 move Shanghai China attend high school move Los Angeles CA attend UCLA Evanston il come msia want datum science field first hear field take statistics econometric course UCLA get fascinate want learn work take datum science course online apply graduate program fully immerse datum science Northwestern visit friend Evanston summer 2015 get visit Northwestern campus think beautiful little know bad winter come MSiA work consultant Deloitte Los Angeles work financial transaction group accounting consulting initial public offering merger acquisition type financial transaction even though non-data-science work opportunity work Deloitte Analytics group use SAS analyze transaction datum various client get interested text analytic take Professor Diego 's class fall use various text analytic method -- regular expression topic modeling tf-idf latent dirichlet allocation -- parse analyze email receive msia use method cluster email recommend past answer future email also practicum gogo use various text analytic method word2vec cluster text immediate plan find company interesting datum product passionate see take\n",
      " \n",
      "CHAORAN WEI name Chaoran Wei MSiA program China undergraduate research project machine learning interested idea solve real world problem datum choose go MSiA get master degree want become data scientist future investigate machine learning two year come MSiA biggest research project attempt classify image datum plankton specific species primary method group use deep Convolutional Neural Networks -lrb- CNN -rrb- research result one conference presentation JMM -lrb- Joint Mathematics meeting -rrb- special session try variety text analytic algorithm undergraduate study use topic modeling technique extract different topic research papers International Relations realm result exciting less interpretable time use LSTM try generate text Wall Street Journal turn model could generate meaningful word even day training try bunch method get good interpretable result part reason want take course get idea model use particular text analytic problem use delving deeper theory nlp future plan become data scientist like idea extract meaningful information pattern large dataset bring business value company feel like solve puzzle money increasingly amount text datum diverse source present web believe text analytic class quarter serve well future career\n",
      " \n",
      "LAUREN YU name Lauren come central China MSiA math major SUNY Buffalo dabble lot social science discover datum science/analytics quantitative social science research lot statistical method deploy although personally engage text analytic project time definitely significant research work interest area Social science offer great range interesting problem involve natural language processing instance text tell author 's intent bias ? measure much therapist empathize patient ? University Washington 's NLP group example collaborate widely political scientist psychologist social scientist tackle kind question remember see paper title `` contextualize sarcasm detection Twitter '' cool ! Sarcasm could subtle nature depend highly subject matter -- many case almost like insider joke able detect algorithmically sound like great idea future hope work project enjoy live city decent public transportation lot korean food earn enough money actually adopt panda bear watch panda video probably biggest addiction something hypnotize bear even roll munch bamboo sleep play keeper play play snow play toy nothing general Pandas spend 14 hour day sleep eat 20lb bamboo shoot day best lifestyle eat sleep repeat ! Ok stop troll reach 300 word already point !\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#retrieve  bottom 3 pairs of most similar bios according to cosine distance: returned 3 pair of indexes (tie situation)\n",
    "for x in range(0,4):\n",
    "    #print(bottom3value[x]) #cosine similarity score\n",
    "    print(\" \".join(list_docs[bottom3_index[0][x]]))\n",
    "    print(\" \")\n",
    "    print(\" \".join(list_docs[bottom3_index[1][x]]))\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### top three similar pair of bios in cosine distance for tf-idf matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       ..., \n",
       "       [False, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False]], dtype=bool)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get top three and bottom three values in tfidf similarity matrix:\n",
    "#get top three values:\n",
    "idx_array_top3value=tfidfSimMatrix.max(0).argsort()[-5:][::-1] #top three value index in array of top value in each document\n",
    "top3value=tfidfSimMatrix.max(0)[idx_array_top3value].tolist()\n",
    "#mark top three values in boolean similarity matrix:\n",
    "top3_tfidfMatrix=np.isin(tfidfSimMatrix, top3value)\n",
    "top3_tfidfMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 7,  8, 20, 22, 22, 28], dtype=int64),\n",
       " array([28, 22, 22,  8, 20,  7], dtype=int64))"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get index of top three values:\n",
    "top3_index=np.where(top3_tfidfMatrix) #first array is row index, second array is column index\n",
    "top3_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEGHAN MENGHAN DING originally Shenzhen southern part China come United State senior year high school Rochester NY 2009 complete undergraduate study Northwestern 2014 degree statistics Economics go work investment management project management 2 year decide take technical path business world come back Northwestern msia program become interested text analytic since join msia program hear term learn see people present work text analytic either class industry amaze amount value extract unstructured datum want learn powerful tool future plan career datum science especially decision science side believe text analytic natural language processing one major distinguishing point make better datum scientist another major reason decide text analytic class pick python MSiA fall love elegant language want become better\n",
      " \n",
      "ERIC PAWLAKOS bear raise Bay Area grow Walnut Creek move around 8th grade bring msia interest statistics undergraduate major Math/Econ find interesting class statistics econometric opportunity within financial industry interested learn data analysis opportunity explore industry program undergraduate student UCLA study math/econ also intern financial firm San Francisco write company valuation report -lrb- potential growth experience management -rrb- determine company invest become interested text analytic one area class program cover also due large amount datum store text seem text analytic quite important data scientist know plan future find job ideally location since spend much time east coast look opportunity need include additional stuff hit 300-600 word limit bit attend UCDavis 2 year UCLA switch major 4 time Davis find campus UCLA one favorite campus play rugby throughout high school college stop play UCLA play guitar since high school mainly casually fun two sibling ; older younger brother older one go college uc Berkeley since graduate younger one start high school\n",
      " \n",
      "MATTHEW FARKAS Hi name Matt Farkas cumming Georgia bear Pennsylvania move Georgia three MSiA undergraduate Georgia Institute Technology study industrial engineering take computational data analysis class work data-centric project undergrad motivate learn datum science apply datum science master 's program though work experience couple internship full time experience expose lightweight analytic recent internship summer MSiA position little bit web scrape visualization Tableau good exposure start thing would learn MSiA text analytic interesting serious interest start learn datum science msia program within program course assignment leverage text technique start show use text analytic really excite course go learn immediate future recently accept full time position company intern summer easy decision ; work interesting people awesome product something use every day excite addition recently start new venture couple classmate excite opportunity well always interested entrepreneurship idea start company addition could actually use text analytic point line venture course could help prepare well\n",
      " \n",
      "MING YU LIU Hi name Ethen MSIA program prior come MSIA program undergrad back Taipei Taiwan Industrial Management National Taiwan University Science Technology reason come program start take couple computer science analytic course junior year find enjoy express logic code work datum derive insight little detour major make certain want learn analytic decide wish pursue advanced degree related field bit research feel like MSIA aim program strike good balance theory apply part datum science thus struggle GRE test apply program thankfully accept become interested text analytic come across method deal cold start problem recommendation system -lrb- note cold start basically user product interact system thus leverage canonical machine learn algorithm prior history work -rrb- scenario product 's item description use feature provide additional side information task make problem still feasible solve realize work knowledge perform text analytic highly beneficial field interested plan near future aim machine learn engineer technical datum scientist role field recommendation system get year industrial hands-on experience technical role maybe consider transition business-oriented datum science role\n",
      " \n",
      "ARON LIU\n",
      " \n",
      "MING YU LIU Hi name Ethen MSIA program prior come MSIA program undergrad back Taipei Taiwan Industrial Management National Taiwan University Science Technology reason come program start take couple computer science analytic course junior year find enjoy express logic code work datum derive insight little detour major make certain want learn analytic decide wish pursue advanced degree related field bit research feel like MSIA aim program strike good balance theory apply part datum science thus struggle GRE test apply program thankfully accept become interested text analytic come across method deal cold start problem recommendation system -lrb- note cold start basically user product interact system thus leverage canonical machine learn algorithm prior history work -rrb- scenario product 's item description use feature provide additional side information task make problem still feasible solve realize work knowledge perform text analytic highly beneficial field interested plan near future aim machine learn engineer technical datum scientist role field recommendation system get year industrial hands-on experience technical role maybe consider transition business-oriented datum science role\n",
      " \n",
      "MING YU LIU Hi name Ethen MSIA program prior come MSIA program undergrad back Taipei Taiwan Industrial Management National Taiwan University Science Technology reason come program start take couple computer science analytic course junior year find enjoy express logic code work datum derive insight little detour major make certain want learn analytic decide wish pursue advanced degree related field bit research feel like MSIA aim program strike good balance theory apply part datum science thus struggle GRE test apply program thankfully accept become interested text analytic come across method deal cold start problem recommendation system -lrb- note cold start basically user product interact system thus leverage canonical machine learn algorithm prior history work -rrb- scenario product 's item description use feature provide additional side information task make problem still feasible solve realize work knowledge perform text analytic highly beneficial field interested plan near future aim machine learn engineer technical datum scientist role field recommendation system get year industrial hands-on experience technical role maybe consider transition business-oriented datum science role\n",
      " \n",
      "MATTHEW FARKAS Hi name Matt Farkas cumming Georgia bear Pennsylvania move Georgia three MSiA undergraduate Georgia Institute Technology study industrial engineering take computational data analysis class work data-centric project undergrad motivate learn datum science apply datum science master 's program though work experience couple internship full time experience expose lightweight analytic recent internship summer MSiA position little bit web scrape visualization Tableau good exposure start thing would learn MSiA text analytic interesting serious interest start learn datum science msia program within program course assignment leverage text technique start show use text analytic really excite course go learn immediate future recently accept full time position company intern summer easy decision ; work interesting people awesome product something use every day excite addition recently start new venture couple classmate excite opportunity well always interested entrepreneurship idea start company addition could actually use text analytic point line venture course could help prepare well\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#retrieve top 3 pairs of most similar bios according to cosine distance: returned four pair of indexes (tie situation)\n",
    "for x in range(0,4):\n",
    "    #print(top3value[x]) #cosine similarity score\n",
    "    print(\" \".join(list_docs[top3_index[0][x]]))\n",
    "    print(\" \")\n",
    "    print(\" \".join(list_docs[top3_index[1][x]]))\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
